{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit Data Collection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzHCgLPOwRZ4HpWtPW+xoi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shakthi-Dhar/Reddit-Data-Collection/blob/main/Reddit_Data_Collection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7pyqIoRerwU"
      },
      "source": [
        "# **Data Collection from Reddit pages**\n",
        "The code below uses PRAW library to extract the reddit submissions and converts it to a suitable format and saved as a .csv file\n",
        "\n",
        "For more details about PRAW check their [documentation](https://praw.readthedocs.io/en/stable/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzZIkSrlfHBo"
      },
      "source": [
        "**Importing the required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpKn3h5_tQDe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b50de3-7703-4aea-eef3-4d6ee52d6c96"
      },
      "source": [
        "!pip install praw\n",
        "!pip install arrow\n",
        "import praw\n",
        "from datetime import datetime\n",
        "import arrow\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.5.0-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting prawcore<3,>=2.1\n",
            "  Downloading prawcore-2.3.0-py3-none-any.whl (16 kB)\n",
            "Collecting websocket-client>=0.54.0\n",
            "  Downloading websocket_client-1.2.3-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.18\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from prawcore<3,>=2.1->praw) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.24.3)\n",
            "Installing collected packages: websocket-client, update-checker, prawcore, praw\n",
            "Successfully installed praw-7.5.0 prawcore-2.3.0 update-checker-0.18.0 websocket-client-1.2.3\n",
            "Collecting arrow\n",
            "  Downloading arrow-1.2.1-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from arrow) (3.10.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.7/dist-packages (from arrow) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.0->arrow) (1.15.0)\n",
            "Installing collected packages: arrow\n",
            "Successfully installed arrow-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeWZL9vdfnA9"
      },
      "source": [
        "**Creating an instance of the Reddit class**\n",
        "\n",
        "Enter your client id, client secret, user agent details\n",
        "\n",
        "Create your own tokens from: [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rORc5vIGtS2g"
      },
      "source": [
        "reddit = praw.Reddit(\n",
        "    # Client ID\n",
        "    client_id = \"\",\n",
        "    # Client Secret Key\n",
        "    client_secret = \"\",\n",
        "    # User Agent\n",
        "    user_agent = \"\",\n",
        "    check_for_async = False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX4352SQlZdj"
      },
      "source": [
        "**Function to convert the given data to CSV format**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Agg8q4C_qXe"
      },
      "source": [
        "def data_csv (data, name):\n",
        "  df = pd.DataFrame(data)\n",
        "  df = df.melt(['date', 'time', 'subreddit', 'search', 'id', 'author', 'title', 'message', 'url'], var_name='Comment No', value_name='Comment Message').sort_values('date', ascending=False)\n",
        "  df['Comment No'] = pd.to_numeric(df['Comment No'].str.split(\" \").str[1])\n",
        "  df = df.sort_values(['date', 'id', 'Comment No'], ascending=[False, True, True])\n",
        "  df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "  df.dropna(subset=[\"Comment Message\"], inplace=True)\n",
        "  df.to_csv(name, index = False)\n",
        "  print(\"The Data has been converted to CSV format successfully.\\nHere is a sample of the dataframe:\\n\")\n",
        "  print(df.head())\n",
        "  print(\"\\nDownload the CSV file.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNLaF1X-kFT8"
      },
      "source": [
        "**Function for extracting the data from given subreddit**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-3UChjMtV14"
      },
      "source": [
        "# Search by subreddits only\n",
        "def subreddit_extraction (subreddit, data):\n",
        "  for submission in reddit.subreddit(subreddit).hot(limit=None):\n",
        "    sub = {}\n",
        "    dt = arrow.get(submission.created_utc).to('local').format()\n",
        "    sub['date'] = dt.split(\" \")[0]\n",
        "    sub['time'] = dt.split(\" \")[1].split(\"+\")[0]\n",
        "    sub['subreddit'] = submission.subreddit.display_name\n",
        "    sub['search'] = \"\"\n",
        "    sub['id'] = submission.id\n",
        "    if(submission.author is not None):\n",
        "      sub['author'] = submission.author.name\n",
        "    else:\n",
        "      sub['author'] = \"\"\n",
        "    sub['title'] = submission.title\n",
        "    sub['message'] = submission.selftext\n",
        "    sub['url'] = submission.url\n",
        "    i = 1\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    for comment in submission.comments.list():\n",
        "      sub['comment '+str(i)] = comment.body\n",
        "      i = i + 1\n",
        "    data.append(sub)\n",
        "  print(\"The data from the given subreddit has been extracted successfully.\\n\")\n",
        "  data_csv(data,subreddit+\"_subreddit.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDslebwelCSR"
      },
      "source": [
        "**Function for extracting the data from a given subreddit and search key word**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8erUneuuvWOE"
      },
      "source": [
        "# Search by subreddits and by a given search element\n",
        "def subreddit_search_extraction (subreddit, data, search):\n",
        "  for submission in reddit.subreddit(subreddit).search(search):\n",
        "    sub = {}\n",
        "    dt = arrow.get(submission.created_utc).to('local').format()\n",
        "    sub['date'] = dt.split(\" \")[0]\n",
        "    sub['time'] = dt.split(\" \")[1].split(\"+\")[0]\n",
        "    sub['subreddit'] = submission.subreddit.display_name\n",
        "    sub['search'] = search\n",
        "    sub['id'] = submission.id\n",
        "    if(submission.author is not None):\n",
        "      sub['author'] = submission.author.name\n",
        "    else:\n",
        "      sub['author'] = \"\"\n",
        "    sub['title'] = submission.title\n",
        "    sub['message'] = submission.selftext\n",
        "    sub['url'] = submission.url\n",
        "    i = 1\n",
        "    submission.comments.replace_more(limit=None)\n",
        "    for comment in submission.comments.list():\n",
        "      sub['comment '+str(i)] = comment.body\n",
        "      i = i + 1\n",
        "    data.append(sub)\n",
        "  print(\"The data from the given subreddit has been extracted successfully.\\n\")\n",
        "  data_csv(data,subreddit+\"_\"+search+\"_subreddit.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlDv6w_Zloa1"
      },
      "source": [
        "**The below code snippet basically calls the functions**\n",
        "\n",
        "Enter your subreddit and search key words here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffLYRHwkgnTf"
      },
      "source": [
        "# Enter the subreddit from which you want to extract the data\n",
        "subreddit = \"technology\"\n",
        "# Enter the search key word for specific data extraction\n",
        "search = \"cloud\"\n",
        "# Create an empty list\n",
        "data = []\n",
        "\n",
        "# Uncomment the below line if you dont have any search optimization\n",
        "subreddit_extraction(subreddit, data)\n",
        "\n",
        "# Uncomment the below line if you have any search key word optimization\n",
        "# subreddit_search_extraction(subreddit, data, search)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}